{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather data taken from http://www.ncdc.noaa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are accessing Global Historical Climatology Network data from here: \n",
    "#https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stations.txt', <email.message.Message at 0xa946668>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt', 'stations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD',\n",
       " 'ACW00011647  17.1333  -61.7833   19.2    ST JOHNS',\n",
       " 'AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196',\n",
       " 'AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194',\n",
       " 'AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217',\n",
       " 'AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218',\n",
       " 'AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930',\n",
       " 'AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938',\n",
       " 'AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948',\n",
       " 'AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the downloaded file.. we will read the first few lines via slicing\n",
    "filepath = 'stations.txt'\n",
    "lines = []\n",
    "with open(filepath) as fp:\n",
    "    lines = list(fp.readlines()[:10])\n",
    "\n",
    "lines = [line.strip() for line in lines]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first ACW00011604 is the id, 17.1167  -61.7833 is the lat,long, 10.1 is the height above sea-level , \n",
    "#'ST JOHNS COOLIDGE FLD' is the name of the meteorological station\n",
    "#some are marked with GSN. GCOS Surface Network (GSN) is a global network of over 1000 stations \n",
    "# selected from the network of many thousands of existing meteorological stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we go through the whole file line by line, get only those marked GSN.. we will collect only the staions names in a dict, \n",
    "# indexed by the station code\n",
    "filepath = 'stations.txt'\n",
    "stations = {}\n",
    "with open(filepath) as fp:\n",
    "    for index, line in enumerate(fp):\n",
    "        if 'GSN' in line:\n",
    "            fields = line.split()\n",
    "            stations[fields[0]] = ' '.join(fields[4:])\n",
    "            \n",
    "\n",
    "len(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there are 994 stations, let's look for interesting patterns within station names and \n",
    "# display only those stations matching that pattern\n",
    "# we will call this function findstation\n",
    "# inside it, lets build a dict using a comprehension, using the station code and names where the pattern we are interested in \n",
    "# is found within the name\n",
    "def findstation(s):\n",
    "    found = {code:name for code,name in stations.items() if s in name}\n",
    "    print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USW00022536': 'HI LIHUE WSO AP 1020.1 GSN 91165'}\n"
     ]
    }
   ],
   "source": [
    "findstation('LIHUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USW00023188': 'CA SAN DIEGO LINDBERGH FLD GSN 72290'}\n"
     ]
    }
   ],
   "source": [
    "findstation('SAN DIEGO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USW00014922': 'MN MINNEAPOLIS/ST PAUL AP GSN HCN 72658'}\n"
     ]
    }
   ],
   "source": [
    "findstation('MINNEAPOLIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RSM00030710': 'IRKUTSK GSN 30710'}\n"
     ]
    }
   ],
   "source": [
    "findstation('IRKUTSK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will focus on these 4 stations from now on\n",
    "datastations = ['USW00022536', 'USW00023188','USW00014922', 'RSM00030710']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USW00022536195002TMAX  256  0  256  0  256  0  267  0  217  0  228  0  256  0  272  0  256  0  256  0  256  0  244  0  256  0  256  0  244  0  244  0  250  0  256  0  239  0  250  0  256  0  256  0  267  0  261  0  267  0  267  0  261  0  261  0-9999   -9999   -9999',\n",
       " 'USW00022536195002TMIN  178  0  156  0  161  0  167  0  167  0  167  0  189  0  211  0  206  0  217  0  217  0  211  0  200  0  200  0  206  0  183  0  206  0  206  0  206  0  194  0  206  0  200  0  206  0  200  0  211  0  183  0  172  0  200  0-9999   -9999   -9999',\n",
       " 'USW00022536195002PRCP    0  0    0  0    0  0    0  0  737  0  406  0   36  0   38  0    0T 0    0T 0    0  0    0T 0   18  0    5  0   10  0   18  0   15  0    5  0    0T 0    0T 0   23  0   10  0    3  0   48  0    0T 0    0T 0    0T 0    5  0-9999   -9999   -9999',\n",
       " 'USW00022536195002SNOW    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0-9999   -9999   -9999',\n",
       " 'USW00022536195002SNWD    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0    0  0-9999   -9999   -9999',\n",
       " 'USW00022536195002WT03-9999   -9999   -9999   -9999       1  0-9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999   -9999',\n",
       " 'USW00022536195002WT16-9999   -9999   -9999   -9999       1  X    1  X    1  X    1  X    1  X    1  X-9999       1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X    1  X-9999   -9999   -9999',\n",
       " 'USW00022536195003TMAX  261  0  261  0  272  0  278  0  250  0  233  0  256  0  272  0  233  0  233  0  239  0  244  0  256  0  261  0  256  0  261  0  272  0  261  0  267  0  244  0  256  0  261  0  250  0  256  0  261  0  256  0  256  0  250  0  244  0  239  0  222  0',\n",
       " 'USW00022536195003TMIN  211  0  172  0  144  0  139  0  156  0  178  0  200  0  156  0  172  0  161  0  178  0  189  0  189  0  200  0  189  0  183  0  200  0  194  0  200  0  194  0  206  0  200  0  194  0  189  0  200  0  194  0  194  0  189  0  178  0  189  0  161  0',\n",
       " 'USW00022536195003PRCP    0T 0    3  0    0  0    0T 0  135  0    0T 0    0  0   41  0  876  0   30  0   20  0    0T 0    0T 0   10  0    0  0    0  0    0  0    3  0    0T 0   25  0    8  0    8  0   18  0   15  0    3  0   91  0   56  0   10  0    5  0    0T 0   61  0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take a look at the data from USW00022536\n",
    "# its present in gsn/USW00022536.dly\n",
    "filepath = 'gsn/USW00022536.dly'\n",
    "lines = []\n",
    "with open(filepath) as fp:\n",
    "    lines = list(fp.readlines()[:10])\n",
    "\n",
    "lines = [line.strip() for line in lines]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can parse the data in USW00022536.dly in the correct format, based on format described in readme.txt\n",
    "# NumPy has a special function, genfromtxt, that can read such a file.\n",
    "# NumPy needs to be told the file name, the size of all the fields, which columns we wish to keep, \n",
    "# the type of all the fields, and last the names that we want to give to all the fields.\n",
    "# The result will be a NumPy record array.\n",
    "def parseFile(filename):\n",
    "    return np.genfromtxt(\n",
    "        filename,\n",
    "        delimiter = dly_delimiter,\n",
    "        usecols = dly_usecols,\n",
    "        dtype = dly_dtype,\n",
    "        names = dly_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for the dly file is in readme.txt, III. FORMAT OF DATA FILES (\".dly\" FILES)\n",
    "# based on info in the table shown in III. FORMAT OF DATA FILES\n",
    "dly_delimiter = [11,4,2,4]+[5,1,1,1]*31 #we know that the fields are these many characters (ex. ID is 1-11 or 11 chars)\n",
    "dly_usecols = [1,2,3]+[4*i for i in range(1,32)] #we will be only using YEAR, MONTH and ELEMENT columns\n",
    "dly_dtype = [np.int32,np.int32,(np.str_,4)] + [np.int32]*31 #here we specify the types of the fields\n",
    "dly_names = ['year','month','obs'] + [str(day) for day in range(1,31+1)] # here we specify the field names.. obs means observable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lihue = parseFile('gsn/USW00022536.dly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(1950, 2, 'TMAX',   256,   256,   256,   267,   217,   228,   256,   272,   256,   256,   256, 244,   256,   256,   244,   244,   250,   256,   239,   250,   256,   256,   267,   261,   267,   267,   261,   261, -9999, -9999, -9999),\n",
       "       (1950, 2, 'TMIN',   178,   156,   161,   167,   167,   167,   189,   211,   206,   217,   217, 211,   200,   200,   206,   183,   206,   206,   206,   194,   206,   200,   206,   200,   211,   183,   172,   200, -9999, -9999, -9999),\n",
       "       (1950, 2, 'PRCP',     0,     0,     0,     0,   737,   406,    36,    38,     0,     0,     0,   0,    18,     5,    10,    18,    15,     5,     0,     0,    23,    10,     3,    48,     0,     0,     0,     5, -9999, -9999, -9999),\n",
       "       ...,\n",
       "       (2018, 9, 'WSF5',   125,   116,   103,   107,   112,   130,   116,    76,    81,    89,   116, 197,   152,   130,   161,   121,   121,   103,    63,    76, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999),\n",
       "       (2018, 9, 'WT01',     1,     1,     1,     1,     1, -9999,     1,     1,     1, -9999,     1,   1,     1, -9999,     1, -9999,     1, -9999, -9999,     1, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999),\n",
       "       (2018, 9, 'WT08', -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999,   1, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999)],\n",
       "      dtype=[('year', '<i4'), ('month', '<i4'), ('obs', '<U4'), ('1', '<i4'), ('2', '<i4'), ('3', '<i4'), ('4', '<i4'), ('5', '<i4'), ('6', '<i4'), ('7', '<i4'), ('8', '<i4'), ('9', '<i4'), ('10', '<i4'), ('11', '<i4'), ('12', '<i4'), ('13', '<i4'), ('14', '<i4'), ('15', '<i4'), ('16', '<i4'), ('17', '<i4'), ('18', '<i4'), ('19', '<i4'), ('20', '<i4'), ('21', '<i4'), ('22', '<i4'), ('23', '<i4'), ('24', '<i4'), ('25', '<i4'), ('26', '<i4'), ('27', '<i4'), ('28', '<i4'), ('29', '<i4'), ('30', '<i4'), ('31', '<i4')])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lihue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a rather complicated numpy record array with all the information contained in the original file\n",
    "# we now need to massage this data into a better form\n",
    "# currently, the temperatures for all the days of the month sit on the same row which \n",
    "# is inconvenient since different months have different days\n",
    "# Instead each day should have a separate row\n",
    "# also I would like to associate each datapoint with the proper numpy datetime object\n",
    "# thus we write a function unroll that applies these transformations\n",
    "# we begin by creating a range of dates that corresponds to a row\n",
    "# we specify the beginning of the month by specifying only the year and the month in a string fed to the numpy.datetime64 object\n",
    "def unroll(record):\n",
    "    startdate = np.datetime64('{}-{:02}'.format(record['year'],record['month'])) \n",
    "    # We then create a range of dates using NumPy arange starting at the start date, ending at the start date plus one month.\n",
    "    # And, with a step of one day.\n",
    "    dates = np.arange(startdate, startdate + np.timedelta64(1,\"M\"), np.timedelta64(1,\"D\"))\n",
    "    \n",
    "    # Next, we are going to collect the data for the days from the record.\n",
    "    # We need to specify the field for which we're going to extract the data.\n",
    "    # We can do that by enclosing dates in enumerate by looping over an index and a date at the same time.\n",
    "    # And, by using that index to build a name of the record called.\n",
    "    # we are also going to divide the datapoints by 10 since temperatures are specified in tenths of degrees. \n",
    "    # note, each row is a tuple\n",
    "    rows = [(date,record[str(i+1)]/10) for i,date in enumerate(dates)]\n",
    "    return rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(numpy.datetime64('1950-02-01'), 25.6),\n",
       " (numpy.datetime64('1950-02-02'), 25.6),\n",
       " (numpy.datetime64('1950-02-03'), 25.6),\n",
       " (numpy.datetime64('1950-02-04'), 26.7),\n",
       " (numpy.datetime64('1950-02-05'), 21.7),\n",
       " (numpy.datetime64('1950-02-06'), 22.8),\n",
       " (numpy.datetime64('1950-02-07'), 25.6),\n",
       " (numpy.datetime64('1950-02-08'), 27.2),\n",
       " (numpy.datetime64('1950-02-09'), 25.6),\n",
       " (numpy.datetime64('1950-02-10'), 25.6),\n",
       " (numpy.datetime64('1950-02-11'), 25.6),\n",
       " (numpy.datetime64('1950-02-12'), 24.4),\n",
       " (numpy.datetime64('1950-02-13'), 25.6),\n",
       " (numpy.datetime64('1950-02-14'), 25.6),\n",
       " (numpy.datetime64('1950-02-15'), 24.4),\n",
       " (numpy.datetime64('1950-02-16'), 24.4),\n",
       " (numpy.datetime64('1950-02-17'), 25.0),\n",
       " (numpy.datetime64('1950-02-18'), 25.6),\n",
       " (numpy.datetime64('1950-02-19'), 23.9),\n",
       " (numpy.datetime64('1950-02-20'), 25.0),\n",
       " (numpy.datetime64('1950-02-21'), 25.6),\n",
       " (numpy.datetime64('1950-02-22'), 25.6),\n",
       " (numpy.datetime64('1950-02-23'), 26.7),\n",
       " (numpy.datetime64('1950-02-24'), 26.1),\n",
       " (numpy.datetime64('1950-02-25'), 26.7),\n",
       " (numpy.datetime64('1950-02-26'), 26.7),\n",
       " (numpy.datetime64('1950-02-27'), 26.1),\n",
       " (numpy.datetime64('1950-02-28'), 26.1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unroll(lihue[0])\n",
    "#so we see that numpy.arange correctly returned the days of February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a list of tuples however, we want to return a proper NumPy record array. We can modify the function to do that.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
